\section{激活函数 Activations}\label{activations}
\subsection{激活函数的用法}

激活函数可以通过设置单独的激活层实现，也可以在构造层对象时通过传递\texttt{activation}参数实现

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from} \NormalTok{keras.layers }\ImportTok{import} \NormalTok{Activation, Dense}

\NormalTok{model.add(Dense(}\DecValTok{64}\NormalTok{))}
\NormalTok{model.add(Activation(}\StringTok{'tanh'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

等价于

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.add(Dense(}\DecValTok{64}\NormalTok{, activation}\OperatorTok{=}\StringTok{'tanh'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

你也可以通过传递一个逐元素运算的Theano/TensorFlow/CNTK函数来作为激活函数：

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from} \NormalTok{keras }\ImportTok{import} \NormalTok{backend }\ImportTok{as} \NormalTok{K}

\NormalTok{model.add(Dense(}\DecValTok{64}\NormalTok{, activation}\OperatorTok{=}\NormalTok{K.tanh))}
\NormalTok{model.add(Activation(K.tanh))}
\end{Highlighting}
\end{Shaded}

\subsection{预定义激活函数}\label{ux9884ux5b9aux4e49ux6fc0ux6d3bux51fdux6570}

\subsubsection{softmax}\label{softmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{softmax(x, axis}\OperatorTok{=-}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Softmax 激活函数.

\textbf{Arguments}

x : 张量. - \textbf{axis}: 整数, 代表softmax所作用的维度

\textbf{Returns}

softmax变换后的张量.

\textbf{Raises}

\begin{itemize}
\tightlist
\item
  \textbf{ValueError}: In case \texttt{dim(x)\ ==\ 1}.
\end{itemize}



\subsubsection{elu}\label{elu}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elu(x, alpha}\OperatorTok{=}\FloatTok{1.0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}



\subsubsection{selu}\label{selu}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selu(x)}
\end{Highlighting}
\end{Shaded}

可伸缩的指数线性单元 (Klambauer et al., 2017)。

\textbf{Arguments}

\begin{itemize}
\tightlist
\item
  \textbf{x}: 一个用来用于计算激活函数的张量或变量。
\end{itemize}

\textbf{Returns}

与\texttt{x}具有相同类型及形状的张量。

\textbf{Note}

\begin{itemize}
\tightlist
\item
  与 "lecun\_normal" 初始化方法一起使用。
\item
  与 dropout 的变种 "AlphaDropout" 一起使用。
\end{itemize}

\textbf{References}

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/1706.02515}{Self-Normalizing Neural
  Networks}
\end{itemize}



\subsubsection{softplus}\label{softplus}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{softplus(x)}
\end{Highlighting}
\end{Shaded}



\subsubsection{softsign}\label{softsign}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{softsign(x)}
\end{Highlighting}
\end{Shaded}



\subsubsection{relu}\label{relu}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{relu(x, alpha}\OperatorTok{=}\FloatTok{0.0}\NormalTok{, max_value}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}



\subsubsection{tanh}\label{tanh}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tanh(x)}
\end{Highlighting}
\end{Shaded}



\subsubsection{sigmoid}\label{sigmoid}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigmoid(x)}
\end{Highlighting}
\end{Shaded}



\subsubsection{hard\_sigmoid}\label{hardux5fsigmoid}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hard_sigmoid(x)}
\end{Highlighting}
\end{Shaded}



\subsubsection{linear}\label{linear}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{linear(x)}
\end{Highlighting}
\end{Shaded}

\subsection{高级激活函数}\label{ux9ad8ux7ea7ux6fc0ux6d3bux51fdux6570}

对于Theano/TensorFlow/CNTK不能表达的复杂激活函数，如含有可学习参数的激活函数，可通过\hyperref[advanced-activations]{高级激活函数}实现，如PReLU，LeakyReLU等
\newpage